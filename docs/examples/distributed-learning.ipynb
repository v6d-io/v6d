{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distributed Learning with Vineyard\n",
    "==================================\n",
    "\n",
    "With the growth of data, distributed learning is becoming a must in real-world machine learning\n",
    "applications, as the data size can easily exceed the memory limit of a single machine.\n",
    "Thus, many distributed systems addressing different workloads are developed\n",
    "and they share the same objective of extending users' single machine prototypes \n",
    "to distributed settings with as few modifications to the code as possible.\n",
    "\n",
    "For example, **dask.dataframe** mimics the API of **pandas** which is the de-facto standard\n",
    "library for single-machine structured data processing, so that users can apply their\n",
    "pandas code for data preprocessing in the dask cluster with few modifications.\n",
    "Similarly, **horovod** provides easy-to-use APIs for users to transfer their single-machine\n",
    "code in machine learning frameworks (e.g., TensorFlow, PyTorch, MXNet) to the distributed settings\n",
    "with only a few additional lines of code.\n",
    "\n",
    "However, when extending to distributed learning, the data sharing between libraries within the same\n",
    "python process (e.g., pandas and tensorflow) becomes inter-process sharing between engines (e.g.,\n",
    "dask and horovod), not to mention in the distributed fashion. Existing solutions using external\n",
    "distributed file systems are less than optimal for the huge I/O overheads.\n",
    "\n",
    "Vineyard shares the same design principle with the aforementioned distributed systems, which aims to\n",
    "provide efficient cross-engine data sharing with few modifications to the existing code.\n",
    "Next, we demonstrate how to transfer a single-machine learing example in **keras** to\n",
    "distributed learning with dask, horovod and Vineyard.\n",
    "\n",
    "An Example from Keras\n",
    "---------------------\n",
    "\n",
    "This [example](https://keras.io/examples/structured_data/wide_deep_cross_networks/)\n",
    "uses the Covertype dataset from the UCI Machine Learning Repository.\n",
    "The task is to predict forest cover type from cartographic variables.\n",
    "The dataset includes 506,011 instances with 12 input features:\n",
    "10 numerical features and 2 categorical features.\n",
    "Each instance is categorized into 1 of 7 classes.\n",
    "\n",
    "The solution contains three steps:\n",
    "\n",
    "1. preprocess the data in pandas to extract the 12 features and the label\n",
    "2. store the preprocessed data in files\n",
    "3. define and train the model in keras\n",
    "\n",
    "\n",
    "Mapping the solution to distributed learning, we have:\n",
    "\n",
    "1. preprocess the data in dask.dataframe\n",
    "2. share the preprocessed data using Vineyard\n",
    "3. train the model in horovod.keras\n",
    "\n",
    "\n",
    "We will walk through the code as follows.\n",
    "\n",
    "Preprocessing the data\n",
    "----------------------\n",
    "\n",
    "Suppose we have a much larger dataset that does not fit into\n",
    "the memory of a single machine. To read the data, we replace\n",
    "**pd.read_csv** by **dd.read_csv**, which will automatically\n",
    "read the data in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "raw_data = dd.read_csv(data_path, header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we preprocess the data using the same code from the example,\n",
    "except the replacement of **pd.concat** to **dd.concat** only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The two categorical features in the dataset are binary-encoded.\n",
    "We will convert this dataset representation to the typical representation, where each\n",
    "categorical feature is represented as a single integer value.\n",
    "\"\"\"\n",
    "\n",
    "soil_type_values = [f\"soil_type_{idx+1}\" for idx in range(40)]\n",
    "wilderness_area_values = [f\"area_type_{idx+1}\" for idx in range(4)]\n",
    "\n",
    "soil_type = raw_data.loc[:, 14:53].apply(\n",
    "    lambda x: soil_type_values[0::1][x.to_numpy().nonzero()[0][0]], axis=1\n",
    ")\n",
    "wilderness_area = raw_data.loc[:, 10:13].apply(\n",
    "    lambda x: wilderness_area_values[0::1][x.to_numpy().nonzero()[0][0]], axis=1\n",
    ")\n",
    "\n",
    "CSV_HEADER = [\n",
    "    \"Elevation\",\n",
    "    \"Aspect\",\n",
    "    \"Slope\",\n",
    "    \"Horizontal_Distance_To_Hydrology\",\n",
    "    \"Vertical_Distance_To_Hydrology\",\n",
    "    \"Horizontal_Distance_To_Roadways\",\n",
    "    \"Hillshade_9am\",\n",
    "    \"Hillshade_Noon\",\n",
    "    \"Hillshade_3pm\",\n",
    "    \"Horizontal_Distance_To_Fire_Points\",\n",
    "    \"Wilderness_Area\",\n",
    "    \"Soil_Type\",\n",
    "    \"Cover_Type\",\n",
    "]\n",
    "\n",
    "data = dd.concat(\n",
    "    [raw_data.loc[:, 0:9], wilderness_area, soil_type, raw_data.loc[:, 54]],\n",
    "    axis=1,\n",
    "    ignore_index=True,\n",
    ")\n",
    "data.columns = CSV_HEADER\n",
    "\n",
    "# Convert the target label indices into a range from 0 to 6 (there are 7 labels in total).\n",
    "data[\"Cover_Type\"] = data[\"Cover_Type\"] - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, instead of saving the preprocessed data into files, we store them in Vineyard.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vineyard\n",
    "import vineyard.contrib.dask.dask # register the dask builders\n",
    "\n",
    "gdf_id = vineyard.connect().put(data, dask_scheduler='tcp://localhost:8786')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ObjectID <\"o00d60aba46eaf536\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saved the preprocessed data as a global dataframe\n",
    "in Vineyard with the ObjectID of **o00d60aba46eaf536**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model\n",
    "------------------\n",
    "\n",
    "In the single machine solution from the example_. A **get_dataset_from_csv** function \n",
    "is defined to load the dataset from the files of the preprocessed data as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_from_csv(csv_file_path, batch_size, shuffle=False):\n",
    "\n",
    "    dataset = tf.data.experimental.make_csv_dataset(\n",
    "        csv_file_path,\n",
    "        batch_size=batch_size,\n",
    "        column_names=CSV_HEADER,\n",
    "        column_defaults=COLUMN_DEFAULTS,\n",
    "        label_name=TARGET_FEATURE_NAME,\n",
    "        num_epochs=1,\n",
    "        header=True,\n",
    "        shuffle=shuffle,\n",
    "    )\n",
    "    return dataset.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "while in the training procedure, it loads the train_dataset and test_dataset\n",
    "seperately from two files as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(model):\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "        metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    "    )\n",
    "\n",
    "    train_dataset = get_dataset_from_csv(train_data_file, batch_size, shuffle=True)\n",
    "\n",
    "    test_dataset = get_dataset_from_csv(test_data_file, batch_size)\n",
    "\n",
    "    print(\"Start training the model...\")\n",
    "    history = model.fit(train_dataset, epochs=num_epochs)\n",
    "    print(\"Model training finished\")\n",
    "\n",
    "    _, accuracy = model.evaluate(test_dataset, verbose=0)\n",
    "\n",
    "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our solution, we provide a function to load dataset from the global dataframe\n",
    "generated in the last step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vineyard.contrib.ml.tensorflow  # register tf data resolvers\n",
    "\n",
    "def get_dataset_from_vineyard(object_id, batch_size, shuffle=False):\n",
    "\n",
    "    ds = vineyard.connect().get(object_id, label=TARGET_FEATURE_NAME) # specify the label column\n",
    "\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(len(ds))\n",
    "\n",
    "    len_test = int(len(ds) * 0.15)\n",
    "    test_dataset = ds.take(len_test).batch(batch_size)\n",
    "    train_dataset = ds.skip(len_test).batch(batch_size)\n",
    "\n",
    "    return train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And modify the training procedure with a few lines of horovod code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(model):\n",
    "\n",
    "    hvd.init()\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=hvd.DistributedOptimizer(keras.optimizers.Adam(learning_rate=learning_rate)),\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "        metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    "    )\n",
    "\n",
    "    callbacks = [\n",
    "        # Horovod: broadcast initial variable states from rank 0 to all other processes.\n",
    "        # This is necessary to ensure consistent initialization of all workers when\n",
    "        # training is started with random weights or restored from a checkpoint.\n",
    "        hvd.callbacks.BroadcastGlobalVariablesCallback(0),\n",
    "    ]\n",
    "\n",
    "    train_dataset, test_dataset = get_dataset_from_vineyard(sys.argv[1], batch_size, shuffle=True)\n",
    "\n",
    "    print(\"Start training the model...\")\n",
    "    history = model.fit(train_dataset, epochs=num_epochs, callbacks=callbacks)\n",
    "    print(\"Model training finished\")\n",
    "\n",
    "    _, accuracy = model.evaluate(test_dataset, verbose=0)\n",
    "\n",
    "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can execute the distributed training with the command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "horovodrun -np 4 -H h1:1,h2:1,h3:1,h4:1 python train.py o00d60aba46eaf536"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the other parts of training procedure are the same as the single machine solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion\n",
    "----------\n",
    "\n",
    "From this example, we can see that with the help of Vineyard, users can easily extend\n",
    "their single machine solutions to distributed learning using dedicated systems without\n",
    "worrying about the cross-system data sharing issues."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
